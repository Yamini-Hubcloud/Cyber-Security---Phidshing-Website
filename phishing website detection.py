# -*- coding: utf-8 -*-
"""DISSERTATION

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nL9dNcZ5bfZk2E1lXIoW3reDLUgnR_qa
"""

!pip install pandas numpy matplotlib seaborn scikit-learn nltk wordcloud tensorflow



import pandas as pd

# Upload a CSV file
from google.colab import files
uploaded = files.upload()

# Read the dataset
PW = pd.read_csv('Training Dataset.csv')


# Display the first few rows
print("First few rows of the dataset:")
print(PW.head())

PW.shape

print(PW.dtypes)

# Data types and summary statistics
print("\nData types and summary statistics:")
print(PW.info())
print(PW.describe())

# Summary of missing values
missing_summary = PW.isnull().sum()
print("Missing values per column:")
print(missing_summary)

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

#  DataFrame is called 'PW'

# -------------------- Missing Value Summary --------------------
missing_counts = PW.isnull().sum()
missing_percents = (missing_counts / len(PW)) * 100
missing_df = pd.DataFrame({
    'Missing Count': missing_counts,
    'Missing %': missing_percents
}).sort_values(by='Missing Count', ascending=False)


# -------------------- Heatmap of Missing Values --------------------
plt.figure(figsize=(12, 6))
sns.heatmap(PW.isnull(), cbar=False, cmap='viridis', yticklabels=False)
plt.title('Heatmap of Missing Data')
plt.xlabel('Columns')
plt.tight_layout()
plt.show()

"""SUMMMARY STATISTICS"""

# Basic statistics: mean, median, std, min, max, percentiles
desc_stats = PW.describe().T
print("\nDescriptive statistics for numeric columns:")
print(desc_stats)

PW.hist(bins=20, figsize=(20, 15), edgecolor='black')
plt.suptitle('Histograms of Numeric Features', fontsize=18)
plt.tight_layout()
plt.show()

# Count values in Result column
label_counts = PW['Result'].value_counts()

# Map labels for clarity
label_names = {1: "Legitimate (+1)", -1: "Phishing (-1)"}
labels = [label_names[i] for i in label_counts.index]

# Pie chart
plt.figure(figsize=(6,6))
plt.pie(label_counts, labels=labels, autopct='%1.1f%%', startangle=90,
        colors=["yellow","red"], explode=(0.05,0.05))
plt.title("Distribution of Legitimate vs Phishing Websites")
plt.show()

# Show counts
label_counts

# Calculate percentage of -1 (phishing indicator) for each variable
phishing_percent = (PW.eq(-1).sum() / len(PW)) * 100

# Convert to dataframe
phishing_summary = phishing_percent.reset_index()
phishing_summary.columns = ["Variable", "Phishing_%"]
phishing_summary = phishing_summary.sort_values(by="Phishing_%", ascending=False)

# Plot horizontal bar chart
plt.figure(figsize=(10,8))
bars = plt.barh(phishing_summary["Variable"], phishing_summary["Phishing_%"], color="pink")
plt.xlabel("Percentage of -1 (Phishing)")
plt.title("Phishing (-1) Percentage per Variable")

# Add percentage labels on bars
for bar in bars:
    plt.text(bar.get_width() + 0.5,             # x-position (slightly outside bar)
             bar.get_y() + bar.get_height()/2, # y-position (middle of bar)
             f"{bar.get_width():.1f}%",        # formatted label
             va='center')

plt.gca().invert_yaxis()  # Highest percentages at top
plt.show()

# Show summary table
phishing_summary

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(35, 25))

# Compute correlation matrix
corr = PW.corr()

# Create upper triangle mask
mask = np.triu(np.ones_like(corr, dtype=bool))

# Plot heatmap
heatmap = sns.heatmap(
    corr,
    mask=mask,
    vmin=-1,
    vmax=1,
    annot=True,
    fmt=".2f",
    cmap='viridis'
)

heatmap.set_title('Full Feature Correlation Heatmap', fontdict={'fontsize': 22}, pad=20)
plt.tight_layout()
plt.savefig('full_correlation_heatmap.png', dpi=300, bbox_inches='tight')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 14))
sns.set(font_scale=1.2)  # consistent larger fonts

# Compute correlation with 'Result' and sort
corr_with_result = PW.corr()[['Result']].sort_values(by='Result', ascending=False)

# Create heatmap
heatmap = sns.heatmap(
    corr_with_result,
    vmin=-1,
    vmax=1,
    annot=True,
    annot_kws={"size":14},
    cmap='viridis',
    cbar=True
)

# Customize title and axes
heatmap.set_title('Feature Correlations with Result', fontdict={'fontsize':20}, pad=16)
heatmap.tick_params(axis='y', labelrotation=0, labelsize=14)  # keep feature labels horizontal
heatmap.tick_params(axis='x', labelsize=14)

plt.tight_layout()
plt.savefig('heatmap_feature_corr.png', dpi=300, bbox_inches='tight')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# List of selected features
selected_features = [
    'SSLfinal_State',        # Strong positive correlation (0.71)
    'URL_of_Anchor',         # Strong positive correlation (0.69)
    'Prefix_Suffix',         # Moderate (0.35)
    'web_traffic',           # Moderate (0.35)
    'having_Sub_Domain',     # Moderate (0.30)
    'Request_URL',           # Moderate (0.25)
    'Links_in_tags',         # Moderate (0.25)
    'SFH'                    # Moderate (0.22)
]

# Loop through each feature and generate stacked bar chart
for feature in selected_features:
    plt.figure(figsize=(6, 4))

    # Generate crosstab (percentages)
    crosstab = pd.crosstab(PW[feature], PW['Result'], normalize='index') * 100

    # Plot stacked bar
    crosstab.plot(kind='bar', stacked=True, colormap='coolwarm', edgecolor='black')

    plt.title(f'{feature} vs Result')
    plt.ylabel('Percentage')
    plt.xlabel(feature)
    plt.xticks(rotation=0)
    plt.legend(title='Result', labels=['Phishing (-1)', 'Legitimate (1)'])
    plt.grid(axis='y', linestyle='--', alpha=0.6)
    plt.tight_layout()

    # Save each plot as PNG
    plt.savefig(f'{feature}_vs_Result.png', dpi=300)
    plt.show()

# List of selected features
selected_features = [
    'having_IP_Address',
'URL_Length',
'Shortining_Service',
'having_At_Symbol',
'double_slash_redirecting',
'Favicon',
'port',
'HTTPS_token',
'Submitting_to_email',
'Abnormal_URL',
'Redirect',
'on_mouseover',
'RightClick',
'popUpWidnow',
'Iframe',
'age_of_domain',
'DNSRecord',
'Page_Rank',
'Google_Index',
'Links_pointing_to_page',
'Statistical_report',

]

# Loop through each feature and generate stacked bar chart
for feature in selected_features:
    plt.figure(figsize=(6, 4))

    # Generate crosstab (percentages)
    crosstab = pd.crosstab(PW[feature], PW['Result'], normalize='index') * 100

    # Plot stacked bar
    crosstab.plot(kind='bar', stacked=True, colormap='coolwarm', edgecolor='black')

    plt.title(f'{feature} vs Result')
    plt.ylabel('Percentage')
    plt.xlabel(feature)
    plt.xticks(rotation=0)
    plt.legend(title='Result', labels=['Phishing (-1)', 'Legitimate (1)'])
    plt.grid(axis='y', linestyle='--', alpha=0.6)
    plt.tight_layout()

    # Save each plot as PNG
    plt.savefig(f'{feature}_vs_Result.png', dpi=300)
    plt.show()

# === ALL CHARTS IN SINGLE LAYOUT ===
import math

# Define subplot grid
n_cols = 4  # adjust for readability (3, 4, or 5)
n_rows = math.ceil(len(selected_features) / n_cols)

fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 4))
axes = axes.flatten()

for idx, feature in enumerate(selected_features):
    ax = axes[idx]

    # Crosstab (percentages)
    crosstab = pd.crosstab(PW[feature], PW['Result'], normalize='index') * 100

    # Plot stacked bar in subplot
    crosstab.plot(
        kind='bar',
        stacked=True,
        colormap='coolwarm',
        edgecolor='black',
        ax=ax,
        legend=False
    )

    ax.set_title(f'{feature} vs Result')
    ax.set_ylabel('Percentage')
    ax.set_xlabel(feature)
    ax.set_xticks(range(len(crosstab.index)))
    ax.set_xticklabels(crosstab.index, rotation=0)
    ax.grid(axis='y', linestyle='--', alpha=0.6)

# Remove empty subplots (if any)
for j in range(len(selected_features), len(axes)):
    fig.delaxes(axes[j])

# One legend for all
handles, labels = ax.get_legend_handles_labels()
fig.legend(handles, ['Phishing (-1)', 'Legitimate (1)'], title="Result", loc="upper center", ncol=2)

plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.show()

# 1. Compare phishing (-1) vs. legitimate (1) sites by feature means
grouped = PW.groupby('Result').mean().T
grouped.columns = ['Phishing (-1)', 'Legitimate (1)']
grouped['Difference'] = grouped['Legitimate (1)'] - grouped['Phishing (-1)']
grouped.sort_values('Difference', ascending=False).head(10)



"""DATA MODELING

(1) Feature Correlation Model
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, ExtraTreesClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
import xgboost as xgb
import warnings
warnings.filterwarnings('ignore')

# Top features (based on your correlation heatmap)
top_features = [
    'SSLfinal_State', 'URL_of_Anchor', 'Prefix_Suffix', 'web_traffic',
    'having_Sub_Domain', 'Request_URL', 'Links_in_tags', 'SFH'
]

# Extract features and target
X = PW[top_features].copy()
y = PW['Result'].copy()

# Map target labels -1 -> 0, 1 -> 1 for binary classification
y = y.map({-1: 0, 1: 1})

# Handle missing values
X = X.fillna(method='ffill')

# Encode categorical features
for col in X.columns:
    if X[col].dtype == 'object':
        X[col] = LabelEncoder().fit_transform(X[col].astype(str))

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale features for certain models
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define models
models = {
    "SVC": SVC(),
    "Decision Tree": DecisionTreeClassifier(),
    "AdaBoost": AdaBoostClassifier(),
    "XGBoost": xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
    "Random Forest": RandomForestClassifier(),
    "Extra Trees": ExtraTreesClassifier(),
    "MLP": MLPClassifier(max_iter=300),
    "KNN": KNeighborsClassifier(),
    "Logistic Regression": LogisticRegression(),
    "LDA": LinearDiscriminantAnalysis()
}

# Train and evaluate
results = {}
for name, model in models.items():
    print(f"\n=== {name} ===")

    if name in ['SVC', 'MLP', 'KNN', 'Logistic Regression', 'LDA']:
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

    # Accuracy
    acc = accuracy_score(y_test, y_pred)
    results[name] = acc
    print(f"Accuracy: {acc:.4f}")

    # Classification report
    print(classification_report(y_test, y_pred))

    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    print("Confusion Matrix:")
    print(cm)

# Print sorted results
print("\n=== Model Performance Ranking ===")
sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)
for i, (name, score) in enumerate(sorted_results, 1):
    print(f"{i}. {name}: {score:.4f}")

"""(2) Category Based Modelling"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, ExtraTreesClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
import xgboost as xgb
import warnings
warnings.filterwarnings('ignore')

# Feature groups
feature_categories = {
    "URL-Based Features": [
        "having_IP_Address", "URL_Length", "Shortining_Service", "having_At_Symbol",
        "double_slash_redirecting", "Prefix_Suffix", "having_Sub_Domain", "HTTPS_token"
    ],
    "Security & Certificate Features": ["SSLfinal_State", "Domain_registeration_length", "age_of_domain"],
    "Visual & UI Behavior Features": ["Favicon", "on_mouseover", "RightClick", "popUpWidnow", "Iframe"],
    "Technical & Network Features": ["port", "DNSRecord", "web_traffic", "Page_Rank", "Google_Index", "Links_pointing_to_page"],
    "Content & Link Behavior Features": ["Request_URL", "URL_of_Anchor", "Links_in_tags", "SFH",
                                        "Submitting_to_email", "Abnormal_URL", "Redirect"],
    "Reputation & External Validation Features": ["Statistical_report"]
}

# Models requiring scaling
scaled_models = ['SVC', 'MLP', 'KNN', 'Logistic Regression', 'LDA']

# Define models
models = {
    "SVC": SVC(),
    "Decision Tree": DecisionTreeClassifier(),
    "AdaBoost": AdaBoostClassifier(),
    "XGBoost": xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
    "Random Forest": RandomForestClassifier(),
    "Extra Trees": ExtraTreesClassifier(),
    "MLP": MLPClassifier(max_iter=300),
    "KNN": KNeighborsClassifier(),
    "Logistic Regression": LogisticRegression(),
    "LDA": LinearDiscriminantAnalysis()
}

# Initialize scaler
scaler = StandardScaler()

# Evaluate all models on each feature group
grouped_model_results = {}

for group_name, features in feature_categories.items():
    print(f"\n=== Feature Group: {group_name} ===")

    # Prepare data
    X = PW[features].copy()
    y = PW['Result'].map({-1: 0, 1: 1})  # Map target to 0/1

    # Handle missing values
    X = X.fillna(method='ffill')

    # Encode categorical columns
    for col in X.columns:
        if X[col].dtype == 'object':
            X[col] = LabelEncoder().fit_transform(X[col].astype(str))

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Scale features if needed
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Store results for this feature group
    results = {}

    for name, model in models.items():
        if name in scaled_models:
            model.fit(X_train_scaled, y_train)
            y_pred = model.predict(X_test_scaled)
        else:
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)

        acc = accuracy_score(y_test, y_pred)
        results[name] = acc

        print(f"\n--- Model: {name} ---")
        print(f"Accuracy: {acc:.4f}")
        print("Classification Report:")
        print(classification_report(y_test, y_pred))
        print("Confusion Matrix:")
        print(confusion_matrix(y_test, y_pred))

    grouped_model_results[group_name] = results

    # Print ranking for this feature group
    sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)
    print("\nModel Performance Ranking:")
    for i, (name, score) in enumerate(sorted_results, 1):
        print(f"{i}. {name}: {score:.4f}")

"""# (3) Data Type Based Grouping"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, ExtraTreesClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
import xgboost as xgb
import warnings

warnings.filterwarnings('ignore')

# Binary and numerical features
binary_features = [
    "having_IP_Address", "Shortining_Service", "having_At_Symbol",
    "double_slash_redirecting", "Prefix_Suffix", "HTTPS_token",
    "SSLfinal_State", "Favicon", "on_mouseover", "RightClick",
    "popUpWidnow", "Iframe", "port", "DNSRecord", "Google_Index",
    "Submitting_to_email", "Abnormal_URL", "Statistical_report"
]

numerical_features = [
    "URL_Length", "having_Sub_Domain", "Domain_registeration_length",
    "age_of_domain", "web_traffic", "Page_Rank", "Links_pointing_to_page",
    "Request_URL", "URL_of_Anchor", "Links_in_tags", "SFH", "Redirect"
]

# Combine features
top_features = binary_features + numerical_features

# Extract features and target
X = PW[top_features].copy()
y = PW['Result'].copy()

# Map target labels -1 -> 0, 1 -> 1 for binary classification
y = y.map({-1: 0, 1: 1})

# Handle missing values
X = X.fillna(method='ffill')

# Encode categorical/binary features if needed
for col in binary_features:
    if X[col].dtype == 'object':
        X[col] = LabelEncoder().fit_transform(X[col].astype(str))

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale features for certain models
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define models
models = {
    "SVC": SVC(),
    "Decision Tree": DecisionTreeClassifier(),
    "AdaBoost": AdaBoostClassifier(),
    "XGBoost": xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
    "Random Forest": RandomForestClassifier(),
    "Extra Trees": ExtraTreesClassifier(),
    "MLP": MLPClassifier(max_iter=300),
    "KNN": KNeighborsClassifier(),
    "Logistic Regression": LogisticRegression(),
    "LDA": LinearDiscriminantAnalysis()
}

# Train and evaluate
results = {}
for name, model in models.items():
    print(f"\n=== {name} ===")

    # Scale features for specific models
    if name in ['SVC', 'MLP', 'KNN', 'Logistic Regression', 'LDA']:
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

    # Accuracy
    acc = accuracy_score(y_test, y_pred)
    results[name] = acc
    print(f"Accuracy: {acc:.4f}")

    # Classification report
    print(classification_report(y_test, y_pred))

    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    print("Confusion Matrix:")
    print(cm)

# Print sorted results
print("\n=== Model Performance Ranking ===")
sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)
for i, (name, score) in enumerate(sorted_results, 1):
    print(f"{i}. {name}: {score:.4f}")

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np


# Step 8: Plot Accuracy Comparison with Trend Line
plt.figure(figsize=(10, 5))
model_names = list(results.keys())
accuracies = list(results.values())

# Bar plot
sns.barplot(x=model_names, y=accuracies, color='skyblue')

# Trend line (connect bars in order)
plt.plot(model_names, accuracies, color='red', marker='o', linewidth=2, label='Trend')

# Annotate each bar with accuracy value
for i, acc in enumerate(accuracies):
    plt.text(i, acc + 0.01, f"{acc:.2f}", ha='center', va='bottom', fontsize=9)

# Labels and formatting
plt.title("Model Accuracy Comparison with Trend Line")
plt.ylabel("Accuracy")
plt.ylim(0, 1)
plt.xticks(rotation=45)
plt.legend()
plt.tight_layout()
plt.show()


# =========================
# Step 12: Confusion Matrix Heatmaps
# =========================
fig, axes = plt.subplots(2, 5, figsize=(20, 10))
axes = axes.ravel()

# Generate predictions (scaled only for models that require it)
all_predictions = {}
for name, model in models.items():
    if name in ['SVC', 'MLP', 'KNN', 'Logistic Regression', 'LDA']:
        all_predictions[name] = model.predict(X_test_scaled)
    else:
        all_predictions[name] = model.predict(X_test)

# Plot confusion matrices
for idx, (name, y_pred) in enumerate(all_predictions.items()):
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[idx],
                xticklabels=["Legit (0)", "Phish (1)"],
                yticklabels=["Legit (0)", "Phish (1)"])
    axes[idx].set_title(name)
    axes[idx].set_xlabel("Predicted")
    axes[idx].set_ylabel("Actual")

# Remove any empty subplots (if models < 10)
for j in range(len(all_predictions), len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

from sklearn.tree import plot_tree
from sklearn.inspection import permutation_importance

# ==============================
# Step 13: Decision Tree (Pruned)
# ==============================
dtree = DecisionTreeClassifier(max_depth=3, min_samples_leaf=5, random_state=42)
dtree.fit(X_train_scaled, y_train)

plt.figure(figsize=(18, 8))
plot_tree(
    dtree,
    feature_names=top_features,   # use your defined features
    class_names=['Legitimate', 'Phishing'],
    filled=True,
    rounded=True,
    fontsize=12
)
plt.title("Decision Tree (Pruned)")
plt.show()


# ==============================
# Step 14: Extra Trees (Single Tree Visualization)
# ==============================
extra_tree_model = ExtraTreesClassifier(
    n_estimators=100,
    max_depth=3,
    min_samples_leaf=5,
    random_state=42
)
extra_tree_model.fit(X_train_scaled, y_train)

# Pick first tree in ensemble
single_tree = extra_tree_model.estimators_[0]

plt.figure(figsize=(18, 8))
plot_tree(
    single_tree,
    feature_names=top_features,
    class_names=['Legitimate', 'Phishing'],
    filled=True,
    rounded=True,
    fontsize=12
)
plt.title("Single Tree from Extra Trees Ensemble (Pruned)")
plt.show()





    # ==============================
# Random Forest & XGBoost Bar Plots (Feature Importance)
# ==============================
feature_names = X_train.columns  # actual features from your DataFrame

for name in ["Random Forest", "XGBoost"]:
    model = models[name]
    plt.figure(figsize=(12, 6))

    # Get feature importances
    importances = model.feature_importances_

    # Create dataframe for plotting
    importance_df = pd.DataFrame({
        "Feature": feature_names,
        "Importance": importances
    }).sort_values(by="Importance", ascending=False)

    # Plot horizontal bar chart
    sns.barplot(x="Importance", y="Feature", data=importance_df, palette="viridis")

    plt.title(f"{name} - Feature Importance")
    plt.xlabel("Importance")
    plt.ylabel("Feature")
    plt.tight_layout()
    plt.show()


# ==============================
# Logistic Regression & LDA Line Charts
# ==============================
for name in ["Logistic Regression", "LDA"]:
    model = models[name]

    if hasattr(model, "coef_"):
        coefs = model.coef_.flatten()

        coef_df = pd.DataFrame({
            "Feature": feature_names,
            "Coefficient": coefs
        }).sort_values(by="Coefficient", ascending=False)

        plt.figure(figsize=(12, 6))
        sns.lineplot(x="Feature", y="Coefficient", data=coef_df, marker="o")

        plt.axhline(0, color="gray", linestyle="--")  # baseline for reference
        plt.title(f"{name} - Coefficients per Feature")
        plt.ylabel("Coefficient Value")
        plt.xticks(rotation=45, ha="right")
        plt.tight_layout()
        plt.show()

from sklearn.inspection import permutation_importance
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# =========================
# Step 13: Feature Importance Plots for Selected Models
# =========================

# Define models and chart types
model_chart_map = {
    "MLP": "bar",
    "KNN": "line",
    "SVC": "scatter",
    "AdaBoost": "scatter"   # updated from violin → scatter
}

# Color palettes for each model
palette_map = {
    "MLP": "viridis",
    "KNN": "coolwarm",
    "SVC": "Reds",
    "AdaBoost": "plasma"
}

# List of all features
all_features = top_features

# Calculate feature importances
feature_importances_selected = {}
for name in model_chart_map.keys():
    model = models[name]

    if hasattr(model, 'feature_importances_'):
        importances = model.feature_importances_
    else:
        # Use permutation importance
        result = permutation_importance(model, X_test_scaled, y_test, n_repeats=10, random_state=42)
        importances = result.importances_mean

    df = pd.DataFrame({
        'Feature': all_features,
        'Importance': importances
    }).sort_values(by='Importance', ascending=False)

    feature_importances_selected[name] = df

# Plot feature importances
plt.figure(figsize=(22, 14))
for idx, (name, chart_type) in enumerate(model_chart_map.items()):
    df = feature_importances_selected[name]
    plt.subplot(2, 2, idx+1)

    if chart_type == "bar":
        sns.barplot(x='Importance', y='Feature', data=df, palette=palette_map[name])
        plt.title(f"{name} - Feature Importance (Bar)", fontsize=14)

    elif chart_type == "line":
        plt.plot(df['Feature'], df['Importance'], marker='o', linestyle='-', color='blue', linewidth=2)
        plt.xticks(rotation=45, ha='right')
        plt.title(f"{name} - Feature Importance (Line)", fontsize=14)
        plt.ylabel("Importance")

    elif chart_type == "scatter":
        # Use seaborn color_palette to get distinct color from palette_map
        color = sns.color_palette(palette_map[name], as_cmap=True)(0.7)
        plt.scatter(df['Feature'], df['Importance'], color=color, s=120, edgecolor='black')
        plt.xticks(rotation=45, ha='right')
        plt.title(f"{name} - Feature Importance (Scatter)", fontsize=14)
        plt.ylabel("Importance")

plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    roc_curve, auc, precision_recall_curve, confusion_matrix,
    accuracy_score, precision_score, recall_score, f1_score, classification_report
)
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler

# ======= Identify which models need scaled data =======
scaled_models = ['SVC', 'MLP', 'KNN', 'Logistic Regression', 'LDA']

# ======= Step 8: ROC Curve + AUC =======
plt.figure(figsize=(10, 7))
all_probabilities = {}

for name, model in models.items():
    X_eval = X_test_scaled if name in scaled_models else X_test
    if hasattr(model, "predict_proba"):
        y_prob = model.predict_proba(X_eval)[:, 1]
    else:
        # fallback for models without predict_proba (e.g., SVC with default)
        y_prob = model.decision_function(X_eval)
        y_prob = (y_prob - y_prob.min()) / (y_prob.max() - y_prob.min())
    all_probabilities[name] = y_prob

    fpr, tpr, _ = roc_curve(y_test, y_prob)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f"{name} (AUC={roc_auc:.2f})")

plt.plot([0, 1], [0, 1], 'k--', label="Random Guess")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curves for All Models")
plt.legend()
plt.show()

# ======= Step 9: Precision-Recall Curve =======
plt.figure(figsize=(10, 7))
for name, y_prob in all_probabilities.items():
    precision, recall, _ = precision_recall_curve(y_test, y_prob)
    plt.plot(recall, precision, label=name)

plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curves for All Models")
plt.legend()
plt.show()

# ======= Step 10: Threshold Tuning for SVC =======
svc_probs = all_probabilities["SVC"]
fpr, tpr, thresholds = roc_curve(y_test, svc_probs)
optimal_idx = np.argmax(tpr - fpr)
optimal_threshold = thresholds[optimal_idx]

print(f"Optimal Threshold for SVC: {optimal_threshold:.3f}")
svc_custom_pred = (svc_probs >= optimal_threshold).astype(int)
print("Classification Report (SVC with Tuned Threshold):")
print(classification_report(y_test, svc_custom_pred))

# ======= Step 11 & 13: 5-Fold Cross-Validation Metrics =======
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_metrics = {}

for name, model in models.items():
    fold_metrics = []
    print(f"\n=== {name} 5-Fold Metrics ===")

    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):
        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]
        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]

        # Scale if needed
        if name in scaled_models:
            scaler = StandardScaler()
            X_train_fold_scaled = scaler.fit_transform(X_train_fold)
            X_val_fold_scaled = scaler.transform(X_val_fold)
        else:
            X_train_fold_scaled, X_val_fold_scaled = X_train_fold, X_val_fold

        # Train
        model.fit(X_train_fold_scaled, y_train_fold)
        y_pred_fold = model.predict(X_val_fold_scaled)

        # Metrics
        acc = accuracy_score(y_val_fold, y_pred_fold)
        prec = precision_score(y_val_fold, y_pred_fold)
        rec = recall_score(y_val_fold, y_pred_fold)
        f1 = f1_score(y_val_fold, y_pred_fold)
        fold_metrics.append((acc, prec, rec, f1))

        print(f"Fold {fold}: Accuracy={acc:.4f}, Precision={prec:.4f}, Recall={rec:.4f}, F1-Score={f1:.4f}")

    fold_metrics = np.array(fold_metrics)
    mean_metrics = fold_metrics.mean(axis=0)
    cv_metrics[name] = mean_metrics
    print(f"Mean (5-Fold): Accuracy={mean_metrics[0]:.4f}, Precision={mean_metrics[1]:.4f}, Recall={mean_metrics[2]:.4f}, F1-Score={mean_metrics[3]:.4f}")

# ======= Step 12: Confusion Matrix Heatmaps =======
import math
n_models = len(models)
n_cols = 3
n_rows = math.ceil(n_models / n_cols)

fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows))
axes = axes.ravel()

all_predictions = {}
for name, model in models.items():
    X_eval = X_test_scaled if name in scaled_models else X_test
    y_pred = model.predict(X_eval)
    all_predictions[name] = y_pred

for idx, (name, y_pred) in enumerate(all_predictions.items()):
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx])
    axes[idx].set_title(name)
    axes[idx].set_xlabel('Predicted')
    axes[idx].set_ylabel('Actual')

# Turn off unused axes
for idx in range(len(all_predictions), len(axes)):
    axes[idx].axis('off')

plt.tight_layout()
plt.show()

# =========================
# Step 17: Top 10 Features Analysis - Cybersecurity Context
# =========================

import numpy as np

# =========================
# Prepare SHAP array
# =========================
if isinstance(shap_values, list):
    if len(shap_values) == 2:
        shap_array = shap_values[1]  # Binary class
    elif len(shap_values) > 2:
        shap_array = shap_values[1]  # Multiclass
else:
    shap_array = shap_values

shap_array = np.array(shap_array)
if shap_array.ndim > 2:
    shap_array = shap_array.mean(axis=1)

# =========================
# Compute mean absolute SHAP values
# =========================
mean_abs_shap = np.abs(shap_array).mean(axis=0)

# =========================
# Get top 10 features
# =========================
top10_idx = np.argsort(mean_abs_shap)[::-1][:10]
top10_features = np.array(all_features)[top10_idx]
top10_values = mean_abs_shap[top10_idx]

# =========================
# Short cybersecurity context
# =========================
feature_context_map = {
    "Prefix_Suffix": " → Hyphens or extra words in URL.",
    "having_Sub_Domain": " → Excessive subdomains mask domain.",
    "Links_in_tags": " → Many embedded links may be malicious.",
    "SFH": " → Suspicious or empty form handlers.",
    "Request_URL": " → Requests to unusual domains.",
    "having_IP_Address": " → Raw IP used instead of domain.",
    "Statistical_report": " → Unusual web content patterns.",
    "popUpWidnow": " → Pop-ups trick user input.",
    "Shortining_Service": " → Hides true URL destination.",
    "on_mouseover": " → JS hover changes link maliciously."
}

# =========================
# Print top 10 features with context
# =========================
print("\n=== Top 10 Most Influential Features ===\n")
for rank, (feat, val) in enumerate(zip(top10_features, top10_values), 1):
    feat_str = str(feat)
    val_float = float(val)
    context = feature_context_map.get(feat_str, "")
    print(f"{rank}. {feat_str} (Mean |SHAP|: {val_float:.4f}){context}")

# =========================
# Step 14: Explainability (SHAP & LIME)
# =========================
import shap
from lime.lime_tabular import LimeTabularExplainer
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Ensure plots are large enough
plt.rcParams["figure.figsize"] = (8, 6)

# Convenience: DataFrames for scaled sets to keep feature names
X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)
X_test_scaled_df  = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)

# =========================
# SHAP Explanation for XGBoost
# =========================
xgb_model = models["XGBoost"]

# SHAP TreeExplainer
explainer = shap.TreeExplainer(xgb_model)

# Compute SHAP values (works with DataFrame)
shap_values = explainer.shap_values(X_test_scaled_df)

# SHAP summary plot (global feature importance)
shap.summary_plot(shap_values, features=X_test_scaled_df, feature_names=all_features)

# SHAP force plot for a single instance
shap.initjs()
i = 5  # index of instance to explain
shap.force_plot(
    explainer.expected_value,
    shap_values[i],
    X_test_scaled_df.iloc[i],
    feature_names=all_features
)

# =========================
# LIME Explanation for SVM
# =========================
# Make sure SVM is trained with probability=True
svm_model = models["SVC"]
if not hasattr(svm_model, "predict_proba"):
    print("Re-training SVM with probability=True for LIME explanations...")
    svm_model = SVC(probability=True)
    svm_model.fit(X_train_scaled, y_train)

# Initialize LIME explainer
lime_explainer = LimeTabularExplainer(
    training_data=X_train_scaled_df.values,
    feature_names=all_features,
    class_names=['Not Phishing', 'Phishing'],
    mode='classification'
)

# Explain a single test instance
i = 10  # index of test sample
lime_exp = lime_explainer.explain_instance(
    X_test_scaled_df.iloc[i].values,
    svm_model.predict_proba
)

# Show explanation in notebook
lime_exp.show_in_notebook(show_table=True)

"""(4)  Phishing Stage Grouping"""

# Step 1: Import Libraries
!pip install pandas scikit-learn xgboost shap lime matplotlib seaborn

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, ExtraTreesClassifier
from xgboost import XGBClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.svm import SVC
import shap
import lime
import lime.lime_tabular
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

# Step 2: Define feature groups
deception_features = [
    "having_IP_Address", "Shortining_Service", "Prefix_Suffix",
    "having_Sub_Domain", "HTTPS_token", "Abnormal_URL"
]

execution_features = [
    "Iframe", "RightClick", "on_mouseover", "popUpWidnow",
    "Request_URL", "Links_in_tags"
]

exfiltration_features = [
    "SFH", "Submitting_to_email", "Redirect", "Statistical_report"
]

all_features = deception_features + execution_features + exfiltration_features

# Step 3: Prepare Data
# Assuming DataFrame `PW` is already loaded with these features and 'Result' column
X = PW[all_features]
y = PW['Result'].map({-1: 0, 1: 1})  # Fix labels for XGBoost & compatibility

# Step 4: Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 5: Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 6: Define Models
models = {
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "AdaBoost": AdaBoostClassifier(random_state=42),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42),
    "Extra Trees": ExtraTreesClassifier(random_state=42),
    "MLP": MLPClassifier(max_iter=300, random_state=42),
    "KNN": KNeighborsClassifier(),
    "Logistic Regression": LogisticRegression(max_iter=300),
    "LDA": LinearDiscriminantAnalysis(),
    "SVM": SVC(probability=True, random_state=42)  # SVM with probability enabled
}



    # Step 7: Train & Evaluate Models with Confusion Matrix
results = {}

for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    acc = accuracy_score(y_test, y_pred)
    results[name] = round(acc, 2)

    print(f"\n=== {name} ===")
    print(f"Accuracy: {acc:.4f}")
    print(classification_report(y_test, y_pred))

    cm = confusion_matrix(y_test, y_pred)
    print("Confusion Matrix:")
    print(cm)

# =========================
# Step 12: Confusion Matrix Heatmaps
# =========================
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Create subplots (2 rows, 5 columns for 10 models)
fig, axes = plt.subplots(2, 5, figsize=(18, 8))
axes = axes.ravel()  # Flatten to 1D array for easy iteration

# Generate predictions for all models
all_predictions = {name: model.predict(X_test_scaled) for name, model in models.items()}

# Loop through each model and plot heatmap
for idx, (name, y_pred) in enumerate(all_predictions.items()):
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx])
    axes[idx].set_title(name, fontsize=12, fontweight='bold')
    axes[idx].set_xlabel("Predicted", fontsize=10)
    axes[idx].set_ylabel("Actual", fontsize=10)

# Adjust layout for better spacing
plt.tight_layout()
plt.show()

# Step 8: Plot Accuracy Comparison with Trend Line
plt.figure(figsize=(10, 5))
model_names = list(results.keys())
accuracies = list(results.values())

sns.barplot(x=model_names, y=accuracies, color='skyblue')
plt.plot(model_names, accuracies, color='red', marker='o', linewidth=2, label='Trend')

for i, acc in enumerate(accuracies):
    plt.text(i, acc + 0.01, f"{acc:.2f}", ha='center', va='bottom')

plt.title("Model Accuracy Comparison with Trend Line")
plt.ylabel("Accuracy")
plt.xticks(rotation=45)
plt.ylim(0, 1)
plt.legend()
plt.tight_layout()
plt.show()

from sklearn.tree import plot_tree
from sklearn.inspection import permutation_importance
from sklearn.tree import plot_tree

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.tree import plot_tree

# ==============================
# Decision Tree & Extra Trees
# ==============================
fig, axes = plt.subplots(1, 2, figsize=(20, 8))

# --- Decision Tree ---
plot_tree(
    dtree,
    feature_names=all_features,
    class_names=['Legitimate', 'Phishing'],
    filled=True, rounded=True, fontsize=10,
    ax=axes[0]
)
axes[0].set_title("Decision Tree (Pruned)")

# --- Extra Trees (single tree) ---
plot_tree(
    single_tree,
    feature_names=all_features,
    class_names=['Legitimate', 'Phishing'],
    filled=True, rounded=True, fontsize=10,
    ax=axes[1]
)
axes[1].set_title("Single Tree from Extra Trees Ensemble (Pruned)")

plt.tight_layout()
plt.show()


# ==============================
# Random Forest & XGBoost Feature Importance
# ==============================
fig, axes = plt.subplots(1, 2, figsize=(20, 6))

for ax, name in zip(axes, ["Random Forest", "XGBoost"]):
    model = models[name]
    importances = model.feature_importances_
    sns.barplot(x=all_features, y=importances, palette='viridis', ax=ax)
    ax.set_title(f"{name} - Feature Importance")
    ax.set_ylabel("Importance")
    ax.tick_params(axis="x", rotation=45)

plt.tight_layout()
plt.show()


# ==============================
# Logistic Regression & LDA Coefficients
# ==============================
fig, axes = plt.subplots(1, 2, figsize=(20, 6))

for ax, name in zip(axes, ["Logistic Regression", "LDA"]):
    model = models[name]
    if hasattr(model, 'coef_'):
        coefs = model.coef_.flatten()
        sns.lineplot(x=all_features, y=coefs, marker='o', ax=ax)
        ax.set_title(f"{name} - Coefficients per Feature")
        ax.set_ylabel("Coefficient Value")
        ax.tick_params(axis="x", rotation=45)

plt.tight_layout()
plt.show()

from sklearn.inspection import permutation_importance
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Models and their chart types
model_chart_map = {
    "MLP": "bar",
    "KNN": "line",
    "SVM": "scatter",
    "AdaBoost": "violin"
}

# Color palettes for each model
palette_map = {
    "MLP": "viridis",
    "KNN": "coolwarm",
    "SVM": "Reds",
    "AdaBoost": "plasma"
}

# Calculate feature importances
feature_importances_selected = {}

for name in model_chart_map.keys():
    model = models[name]

    if hasattr(model, 'feature_importances_'):
        importances = model.feature_importances_
    else:
        result = permutation_importance(model, X_test_scaled, y_test, n_repeats=10, random_state=42)
        importances = result.importances_mean

    df = pd.DataFrame({
        'Feature': all_features,
        'Importance': importances
    }).sort_values(by='Importance', ascending=False)

    feature_importances_selected[name] = df

# Plot each model with its designated chart
plt.figure(figsize=(22, 14))

for idx, (name, chart_type) in enumerate(model_chart_map.items()):
    df = feature_importances_selected[name]
    plt.subplot(2, 2, idx+1)

    if chart_type == "bar":
        sns.barplot(x='Importance', y='Feature', data=df, palette=palette_map[name])
        plt.title(f"{name} - Feature Importance (Bar)", fontsize=14)
    elif chart_type == "line":
        plt.plot(df['Feature'], df['Importance'], marker='o', linestyle='-', color='blue', linewidth=2)
        plt.xticks(rotation=45, ha='right')
        plt.title(f"{name} - Feature Importance (Line)", fontsize=14)
        plt.ylabel("Importance")
    elif chart_type == "scatter":
        plt.scatter(df['Feature'], df['Importance'], color='red', s=120, edgecolor='black')
        plt.xticks(rotation=45, ha='right')
        plt.title(f"{name} - Feature Importance (Scatter)", fontsize=14)
        plt.ylabel("Importance")
    elif chart_type == "violin":
        sns.violinplot(x='Importance', y='Feature', data=df, palette=palette_map[name], orient='h')
        plt.title(f"{name} - Feature Importance (Violin)", fontsize=14)

plt.tight_layout()
plt.show()

from sklearn.metrics import roc_curve, auc, precision_recall_curve, confusion_matrix
from sklearn.model_selection import cross_val_score, StratifiedKFold

# =========================
# Step 8: ROC Curve + AUC
# =========================
plt.figure(figsize=(8, 6))
all_probabilities = {name: model.predict_proba(X_test_scaled)[:,1] for name, model in models.items()}

for name, y_prob in all_probabilities.items():
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f"{name} (AUC={roc_auc:.2f})")

plt.plot([0, 1], [0, 1], 'k--', label="Random Guess")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curves for All Models")
plt.legend()
plt.show()

# =========================
# Step 9: Precision-Recall Curve
# =========================
plt.figure(figsize=(8, 6))
for name, y_prob in all_probabilities.items():
    precision, recall, _ = precision_recall_curve(y_test, y_prob)
    plt.plot(recall, precision, label=name)

plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curves for All Models")
plt.legend()
plt.show()

# =========================
# Step 10: Threshold Tuning for SVM
# =========================
svm_probs = all_probabilities["SVM"]
fpr, tpr, thresholds = roc_curve(y_test, svm_probs)
optimal_idx = np.argmax(tpr - fpr)
optimal_threshold = thresholds[optimal_idx]

print(f"Optimal Threshold for SVM: {optimal_threshold:.3f}")
svm_custom_pred = (svm_probs >= optimal_threshold).astype(int)
print("Classification Report (SVM with Tuned Threshold):")
print(classification_report(y_test, svm_custom_pred))

# =========================
# Step 11: Cross-Validation
# =========================
cv_results = {}
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for name, model in models.items():
    scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')
    cv_results[name] = scores.mean()
    print(f"{name}: Mean CV Accuracy = {scores.mean():.4f}")

    from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import precision_score, recall_score, f1_score

# =========================
# Step 13: 5-Fold Cross-Validation Metrics
# =========================
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

cv_metrics = {}

for name, model in models.items():
    fold_metrics = []
    print(f"\n=== {name} 5-Fold Metrics ===")

    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):
        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]
        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]

        # Scale features
        scaler = StandardScaler()
        X_train_fold_scaled = scaler.fit_transform(X_train_fold)
        X_val_fold_scaled = scaler.transform(X_val_fold)

        # Train model
        model.fit(X_train_fold_scaled, y_train_fold)
        y_pred_fold = model.predict(X_val_fold_scaled)

        # Calculate metrics
        acc = accuracy_score(y_val_fold, y_pred_fold)
        prec = precision_score(y_val_fold, y_pred_fold)
        rec = recall_score(y_val_fold, y_pred_fold)
        f1 = f1_score(y_val_fold, y_pred_fold)

        fold_metrics.append((acc, prec, rec, f1))
        print(f"Fold {fold}: Accuracy={acc:.4f}, Precision={prec:.4f}, Recall={rec:.4f}, F1-Score={f1:.4f}")

    # Average metrics across folds
    fold_metrics = np.array(fold_metrics)
    mean_metrics = fold_metrics.mean(axis=0)
    cv_metrics[name] = mean_metrics
    print(f"Mean (5-Fold): Accuracy={mean_metrics[0]:.4f}, Precision={mean_metrics[1]:.4f}, Recall={mean_metrics[2]:.4f}, F1-Score={mean_metrics[3]:.4f}")


# =========================
# Step 12: Confusion Matrix Heatmaps
# =========================
fig, axes = plt.subplots(2, 5, figsize=(18, 8))
axes = axes.ravel()

all_predictions = {name: model.predict(X_test_scaled) for name, model in models.items()}

for idx, (name, y_pred) in enumerate(all_predictions.items()):
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx])
    axes[idx].set_title(name)
    axes[idx].set_xlabel('Predicted')
    axes[idx].set_ylabel('Actual')

plt.tight_layout()
plt.show()

# =========================
# Step 14: Imports (Explainability)
# =========================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import shap
from lime.lime_tabular import LimeTabularExplainer

from sklearn.inspection import permutation_importance, PartialDependenceDisplay
from sklearn.metrics import classification_report

# If your plots pop up too small in some environments:
plt.rcParams["figure.figsize"] = (8, 6)

# Convenience: DataFrames for scaled sets to keep feature names in plots
X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)
X_test_scaled_df  = pd.DataFrame(X_test_scaled,  columns=X.columns, index=X_test.index)

# Pull final models from your dict
rf_model  = models["Random Forest"]
xgb_model = models.get("XGBoost", None)  # optional if you want to use it later
svm_model = models["SVM"]

# =========================
# Step 15: Permutation Importance (SVM)
# =========================
# Model-agnostic feature importance for SVM using F1 as the scoring metric.
perm_result = permutation_importance(
    svm_model,
    X_test_scaled_df,     # SVM expects scaled features
    y_test,
    scoring="f1",         # choose 'roc_auc' or 'accuracy' if preferred
    n_repeats=20,
    random_state=42,
    n_jobs=-1
)

perm_importances = pd.Series(perm_result.importances_mean, index=X.columns).sort_values(ascending=False)
perm_std = pd.Series(perm_result.importances_std, index=X.columns).loc[perm_importances.index]

print("\nTop 20 Permutation Importances (SVM, scoring=F1):")
display(perm_importances.head(20))

# Plot top 20
topN = 20
plt.figure(figsize=(9, 7))
perm_importances.head(topN).plot(kind="bar", yerr=perm_std.head(topN))
plt.title("Permutation Importance (SVM)")
plt.ylabel("Mean Importance (F1 drop)")
plt.xlabel("Feature")
plt.tight_layout()
plt.show()

# =========================
# Step 9: SHAP Explanation (XGBoost )
# =========================
import shap

# Initialize SHAP explainer for XGBoost
explainer = shap.TreeExplainer(models["XGBoost"])

# Compute SHAP values for test set
shap_values = explainer.shap_values(X_test_scaled)

# SHAP Summary Plot (global feature importance)
shap.summary_plot(shap_values, features=X_test_scaled, feature_names=all_features)

# SHAP Force Plot for a single instance
shap.initjs()  # Enable JS visualization
i = 5  # Index of instance to explain
shap.force_plot(
    explainer.expected_value,
    shap_values[i],
    X_test_scaled[i],
    feature_names=all_features
)

# =========================
# Step 10: LIME Explanation (SVM )
# =========================
from lime.lime_tabular import LimeTabularExplainer

# Initialize LIME explainer for tabular data
lime_explainer = LimeTabularExplainer(
    training_data=X_train_scaled,
    feature_names=all_features,
    class_names=['Not Phishing', 'Phishing'],
    mode='classification'
)

# Explain a single test instance
i = 10  # Index of test sample
lime_exp = lime_explainer.explain_instance(
    X_test_scaled[i],
    models["SVM"].predict_proba
)

# Display explanation
lime_exp.show_in_notebook(show_table=True)

"""# (5) Internal vs External Features"""

# Run this in Google Colab

# 1. Install required packages
!pip install xgboost shap lime --quiet

# 2. Import libraries
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report

from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, ExtraTreesClassifier
from xgboost import XGBClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

import shap
import lime
import lime.lime_tabular

# 3. Define features based on your input
internal_features = [
    "having_IP_Address", "URL_Length", "Shortining_Service", "having_At_Symbol",
    "double_slash_redirecting", "Prefix_Suffix", "having_Sub_Domain", "HTTPS_token",
    "SSLfinal_State", "Favicon", "on_mouseover", "RightClick", "popUpWindoww", "Iframe",
    "port", "DNSRecord", "Request_URL", "URL_of_Anchor", "Links_in_tags",
    "SFH", "Submitting_to_email", "Abnormal_URL", "Redirect"
]

external_features = [
    "web_traffic", "Page_Rank", "Google_Index", "Domain_registeration_length",
    "age_of_domain", "Links_pointing_to_page", "Statistical_report"
]

all_features = internal_features + external_features

# 4. Load your dataframe here - placeholder example:
# PW = pd.read_csv('your_data.csv')  # Replace with your actual data load code

# For demo, create dummy data (REMOVE this in your actual run):
np.random.seed(42)
PW = pd.DataFrame(np.random.randn(1000, len(all_features)), columns=all_features)
PW['label'] = np.random.choice([-1, 1], size=1000)  # Labels as -1 and 1

# 5. Fix label encoding: map -1 to 0 for compatibility with classifiers (especially XGBoost)
PW['label'] = PW['label'].replace(-1, 0)

# 6. Split data
X = PW[all_features]
y = PW['label']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 7. Scale features (some models benefit from scaling)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 8. Initialize models
models = {
    "SVC": SVC(probability=True, random_state=42),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "AdaBoost": AdaBoostClassifier(random_state=42),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42),
    "Extra Trees": ExtraTreesClassifier(random_state=42),
    "MLP": MLPClassifier(max_iter=500, random_state=42),
    "KNN": KNeighborsClassifier(),
    "Logistic Regression": LogisticRegression(max_iter=500, random_state=42),
    "LDA": LinearDiscriminantAnalysis()
}

# 9. Train and evaluate models
results = {}

for name, model in models.items():
    print(f"\nTraining and evaluating: {name}")
    # For models sensitive to scaling, use scaled data
    if name in ["SVC", "MLP", "Logistic Regression", "KNN", "LDA"]:
        model.fit(X_train_scaled, y_train)
        preds = model.predict(X_test_scaled)
    else:
        model.fit(X_train, y_train)
        preds = model.predict(X_test)

    acc = accuracy_score(y_test, preds)
    print(f"Accuracy: {acc:.4f}")
    print("Classification Report:")
    print(classification_report(y_test, preds))
    results[name] = model

# 10. Model interpretability for Random Forest using SHAP & LIME

# SHAP
print("\nGenerating SHAP explanation for Random Forest...")
explainer = shap.TreeExplainer(results["Random Forest"])
shap_values = explainer.shap_values(X_test)

# Uncomment below line to show interactive SHAP summary plot in notebook
# shap.summary_plot(shap_values[1], X_test, feature_names=all_features)

# LIME
print("\nGenerating LIME explanation for Random Forest...")
lime_explainer = lime.lime_tabular.LimeTabularExplainer(
    training_data=np.array(X_train),
    feature_names=all_features,
    class_names=['Legitimate', 'Phishing'],
    mode='classification'
)

# Explain a test instance (e.g. first test row)
i = 0
exp = lime_explainer.explain_instance(
    data_row=X_test.iloc[i].values,
    predict_fn=results["Random Forest"].predict_proba
)

print(f"LIME explanation for test instance {i}:")
exp.show_in_notebook(show_table=True)
